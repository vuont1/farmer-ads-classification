{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T08:10:46.586091Z",
     "start_time": "2025-05-29T08:10:28.582081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4143 text samples with labels: [-1  1]\n",
      "\n",
      "Cleaned Texts:\n",
      "Sample 1: ad-jerry ad-bruckheimer ad-chase ad-premier ad-sept ad-th ad-clip ad-bruckheimer ad-chase page found\n",
      "Sample 2: ad-rheumatoid ad-arthritis ad-expert ad-tip ad-info ad-article ad-treatment ad-option ad-support title-understand title-rheumatoid title-arthritis title-everyday title-health header-understand header-rheumatoid header-arthritis understand rheumatoid arthritis everyday health root root act consumer root content everyday solution understand rheumatoid arthritis future ra treatment advance rheumatoid arthritis treatment expect future lead researcher ra treatment research exercise ra check tip slideshow help create workout program ra fitness tip question doctor print list rheumatoid arthritis question doctor visit list ra question understand rheumatoid arthritis tip manage rheumatoid arthritis pain mak key change help manage rheumatoid arthritis pain ease joint pain strive eat balance diet help healthy weight sufficient vitamin mineral counter chronic inflammation tip ease rheumatoid arthritis pain cause rheumatoid arthritis inflammation rheumatoid arthritis ra symptom cause inflammation learn inflammation lead ra symptom joint pain stiffness plus discover cause rheumatoid arthritis rheumatoid arthritis inflammation rheumatoid arthritis management research rheumatoid arthritis research lifestyle choice day impact ra symptom example people ra experience depression treate depression help people manage ra control ra rheumatoid arthritis expert yoga ra yoga safe exercise option person rheumatoid arthritis read dr susan lee answer root act consumer rheumatoid arthritis poll lifestyle change help manage ra pain please select option eat healthy balance diet muscle strengthen aerobic exercise sleep night try reduce manage stress technique haven lifestyle change toolkit healthy recipe shop list meal planner recipe box tool root act consumer enter search term register sign newsletter home health common condition add adhd addiction allergy alternative health alzheimer disease anxiety disorder arthritis asthma autism autoimmune disorder bipolar disorder pain breast cancer cancer cardiovascular health cold flu dental health depression diabete diet nutrition digestive health dvt emotional health epilepsy erectile dysfunction family health fibromyalgia fitness gerd headache migraine healthy home healthy live heart health cholesterol hiv aid hypertension ib incontinence kid health health menopause multiple sclerosis osteoporosis pain management pet health psoriasis rheumatoid arthritis schizophrenia senior health sexual health skin beauty sleep stop smok stroke swine flu weight women health yeast infection condition drug symptom checker flu checkup abdominal pain arm pain pain body ach breast pain breathing difficulty chest pain congestion cough diarrhea ear pain excessive sweate faintness fatigue fever ga headache irregular period joint pain leg pain mouth lesion nausea neck pain rash rectal bleed skin lump sore throat vaginal itch vomite food fitness calorie counter healthy recipe search recipe diet nutrition weight fitness community profile blog discussion photo albums everyday health health tool bmi calculator bmr calculator body fat calculator brain game conversion calculator glossary glucose tracker meal planner calorie counter photo gallery recipe box symptom checker video weight tracker everyday health edit profile inbox discussion blog friend tool copyright everyday health inc everydayhealth com everyday health inc help ad policy advertise link feedback advertise notice site third party advertisement site collect information visit site website provide advertisement service obtain information advertise practice choice online behavioral advertise please click material web site provide educational purpose medical advice diagnosis treatment additional information site subject term privacy policy site comply honcode standard trustworthy health information verify\n",
      "Sample 3: ad-rheumatologist ad-anju ad-varghese ad-yonker ad-ny ad-pomona ad-ny ad-www ad-arthritisandrheumatologyny ad-com title-practice title-location practice location arthritis rheumatology pllc rheumatology specialist anju varghese board certify internal medicine rheumatology practice limit rheumatology subtitle text home practice location patient resource rockland county ny medical park dr lower level pomona ny phone fax route exit palisad parkway westchester county ny north broadway nd floor yonker ny phone fax st john riverside hospital exit sawmill river parkway accept patient participate health insurance health plan content copyright host exchange\n",
      "Sample 4: ad-siemen ad-water ad-remediation ad-water ad-scarce ad-resource ad-siemen ad-help ad-preserve title-siemen title-usa siemen usa skip content siemen skip site identifier siemen usa close site id layer skip language selection skip generic navigation contact skip search search industry energy healthcare business product industry solution motor drive build technology industry automation financial solution solution service lighte osram sylvania product lifecycle management mobility water technology power generation power transmission power distribution automation control protection electrical compression expansion ventilation mechanical drive service financial solution solution service diagnostic image therapy hear aid product laboratory diagnostics build technology financial solution solution service consumer product corporate research government solution information communication siemen financial solution solution service siemen usa investor relation press job career business siemen global website answer america renewable energy smart grid technology medical image electronic healthcare record green build commuter rail system employee siemen unit commit answer america toughest question close productfinder layer close logo layer siemen corporation corporate information privacy policy term digital id\n",
      "Sample 5: ad-symptom ad-muscle ad-weakness ad-genetic ad-disease ad-symptom ad-include ad-search ad-learn page found\n",
      "\n",
      "Tokenized Texts:\n",
      "Sample 1: ['ad-jerry', 'ad-bruckheimer', 'ad-chase', 'ad-premier', 'ad-sept', 'ad-th', 'ad-clip', 'ad-bruckheimer', 'ad-chase', 'page', 'found']\n",
      "Sample 2: ['ad-rheumatoid', 'ad-arthritis', 'ad-expert', 'ad-tip', 'ad-info', 'ad-article', 'ad-treatment', 'ad-option', 'ad-support', 'title-understand', 'title-rheumatoid', 'title-arthritis', 'title-everyday', 'title-health', 'header-understand', 'header-rheumatoid', 'header-arthritis', 'understand', 'rheumatoid', 'arthritis', 'everyday', 'health', 'root', 'root', 'act', 'consumer', 'root', 'content', 'everyday', 'solution', 'understand', 'rheumatoid', 'arthritis', 'future', 'ra', 'treatment', 'advance', 'rheumatoid', 'arthritis', 'treatment', 'expect', 'future', 'lead', 'researcher', 'ra', 'treatment', 'research', 'exercise', 'ra', 'check', 'tip', 'slideshow', 'help', 'create', 'workout', 'program', 'ra', 'fitness', 'tip', 'question', 'doctor', 'print', 'list', 'rheumatoid', 'arthritis', 'question', 'doctor', 'visit', 'list', 'ra', 'question', 'understand', 'rheumatoid', 'arthritis', 'tip', 'manage', 'rheumatoid', 'arthritis', 'pain', 'mak', 'key', 'change', 'help', 'manage', 'rheumatoid', 'arthritis', 'pain', 'ease', 'joint', 'pain', 'strive', 'eat', 'balance', 'diet', 'help', 'healthy', 'weight', 'sufficient', 'vitamin', 'mineral', 'counter', 'chronic', 'inflammation', 'tip', 'ease', 'rheumatoid', 'arthritis', 'pain', 'cause', 'rheumatoid', 'arthritis', 'inflammation', 'rheumatoid', 'arthritis', 'ra', 'symptom', 'cause', 'inflammation', 'learn', 'inflammation', 'lead', 'ra', 'symptom', 'joint', 'pain', 'stiffness', 'plus', 'discover', 'cause', 'rheumatoid', 'arthritis', 'rheumatoid', 'arthritis', 'inflammation', 'rheumatoid', 'arthritis', 'management', 'research', 'rheumatoid', 'arthritis', 'research', 'lifestyle', 'choice', 'day', 'impact', 'ra', 'symptom', 'example', 'people', 'ra', 'experience', 'depression', 'treate', 'depression', 'help', 'people', 'manage', 'ra', 'control', 'ra', 'rheumatoid', 'arthritis', 'expert', 'yoga', 'ra', 'yoga', 'safe', 'exercise', 'option', 'person', 'rheumatoid', 'arthritis', 'read', 'dr', 'susan', 'lee', 'answer', 'root', 'act', 'consumer', 'rheumatoid', 'arthritis', 'poll', 'lifestyle', 'change', 'help', 'manage', 'ra', 'pain', 'please', 'select', 'option', 'eat', 'healthy', 'balance', 'diet', 'muscle', 'strengthen', 'aerobic', 'exercise', 'sleep', 'night', 'try', 'reduce', 'manage', 'stress', 'technique', 'haven', 'lifestyle', 'change', 'toolkit', 'healthy', 'recipe', 'shop', 'list', 'meal', 'planner', 'recipe', 'box', 'tool', 'root', 'act', 'consumer', 'enter', 'search', 'term', 'register', 'sign', 'newsletter', 'home', 'health', 'common', 'condition', 'add', 'adhd', 'addiction', 'allergy', 'alternative', 'health', 'alzheimer', 'disease', 'anxiety', 'disorder', 'arthritis', 'asthma', 'autism', 'autoimmune', 'disorder', 'bipolar', 'disorder', 'pain', 'breast', 'cancer', 'cancer', 'cardiovascular', 'health', 'cold', 'flu', 'dental', 'health', 'depression', 'diabete', 'diet', 'nutrition', 'digestive', 'health', 'dvt', 'emotional', 'health', 'epilepsy', 'erectile', 'dysfunction', 'family', 'health', 'fibromyalgia', 'fitness', 'gerd', 'headache', 'migraine', 'healthy', 'home', 'healthy', 'live', 'heart', 'health', 'cholesterol', 'hiv', 'aid', 'hypertension', 'ib', 'incontinence', 'kid', 'health', 'health', 'menopause', 'multiple', 'sclerosis', 'osteoporosis', 'pain', 'management', 'pet', 'health', 'psoriasis', 'rheumatoid', 'arthritis', 'schizophrenia', 'senior', 'health', 'sexual', 'health', 'skin', 'beauty', 'sleep', 'stop', 'smok', 'stroke', 'swine', 'flu', 'weight', 'women', 'health', 'yeast', 'infection', 'condition', 'drug', 'symptom', 'checker', 'flu', 'checkup', 'abdominal', 'pain', 'arm', 'pain', 'pain', 'body', 'ach', 'breast', 'pain', 'breathing', 'difficulty', 'chest', 'pain', 'congestion', 'cough', 'diarrhea', 'ear', 'pain', 'excessive', 'sweate', 'faintness', 'fatigue', 'fever', 'ga', 'headache', 'irregular', 'period', 'joint', 'pain', 'leg', 'pain', 'mouth', 'lesion', 'nausea', 'neck', 'pain', 'rash', 'rectal', 'bleed', 'skin', 'lump', 'sore', 'throat', 'vaginal', 'itch', 'vomite', 'food', 'fitness', 'calorie', 'counter', 'healthy', 'recipe', 'search', 'recipe', 'diet', 'nutrition', 'weight', 'fitness', 'community', 'profile', 'blog', 'discussion', 'photo', 'albums', 'everyday', 'health', 'health', 'tool', 'bmi', 'calculator', 'bmr', 'calculator', 'body', 'fat', 'calculator', 'brain', 'game', 'conversion', 'calculator', 'glossary', 'glucose', 'tracker', 'meal', 'planner', 'calorie', 'counter', 'photo', 'gallery', 'recipe', 'box', 'symptom', 'checker', 'video', 'weight', 'tracker', 'everyday', 'health', 'edit', 'profile', 'inbox', 'discussion', 'blog', 'friend', 'tool', 'copyright', 'everyday', 'health', 'inc', 'everydayhealth', 'com', 'everyday', 'health', 'inc', 'help', 'ad', 'policy', 'advertise', 'link', 'feedback', 'advertise', 'notice', 'site', 'third', 'party', 'advertisement', 'site', 'collect', 'information', 'visit', 'site', 'website', 'provide', 'advertisement', 'service', 'obtain', 'information', 'advertise', 'practice', 'choice', 'online', 'behavioral', 'advertise', 'please', 'click', 'material', 'web', 'site', 'provide', 'educational', 'purpose', 'medical', 'advice', 'diagnosis', 'treatment', 'additional', 'information', 'site', 'subject', 'term', 'privacy', 'policy', 'site', 'comply', 'honcode', 'standard', 'trustworthy', 'health', 'information', 'verify']\n",
      "Sample 3: ['ad-rheumatologist', 'ad-anju', 'ad-varghese', 'ad-yonker', 'ad-ny', 'ad-pomona', 'ad-ny', 'ad-www', 'ad-arthritisandrheumatologyny', 'ad-com', 'title-practice', 'title-location', 'practice', 'location', 'arthritis', 'rheumatology', 'pllc', 'rheumatology', 'specialist', 'anju', 'varghese', 'board', 'certify', 'internal', 'medicine', 'rheumatology', 'practice', 'limit', 'rheumatology', 'subtitle', 'text', 'home', 'practice', 'location', 'patient', 'resource', 'rockland', 'county', 'ny', 'medical', 'park', 'dr', 'lower', 'level', 'pomona', 'ny', 'phone', 'fax', 'route', 'exit', 'palisad', 'parkway', 'westchester', 'county', 'ny', 'north', 'broadway', 'nd', 'floor', 'yonker', 'ny', 'phone', 'fax', 'st', 'john', 'riverside', 'hospital', 'exit', 'sawmill', 'river', 'parkway', 'accept', 'patient', 'participate', 'health', 'insurance', 'health', 'plan', 'content', 'copyright', 'host', 'exchange']\n",
      "Sample 4: ['ad-siemen', 'ad-water', 'ad-remediation', 'ad-water', 'ad-scarce', 'ad-resource', 'ad-siemen', 'ad-help', 'ad-preserve', 'title-siemen', 'title-usa', 'siemen', 'usa', 'skip', 'content', 'siemen', 'skip', 'site', 'identifier', 'siemen', 'usa', 'close', 'site', 'id', 'layer', 'skip', 'language', 'selection', 'skip', 'generic', 'navigation', 'contact', 'skip', 'search', 'search', 'industry', 'energy', 'healthcare', 'business', 'product', 'industry', 'solution', 'motor', 'drive', 'build', 'technology', 'industry', 'automation', 'financial', 'solution', 'solution', 'service', 'lighte', 'osram', 'sylvania', 'product', 'lifecycle', 'management', 'mobility', 'water', 'technology', 'power', 'generation', 'power', 'transmission', 'power', 'distribution', 'automation', 'control', 'protection', 'electrical', 'compression', 'expansion', 'ventilation', 'mechanical', 'drive', 'service', 'financial', 'solution', 'solution', 'service', 'diagnostic', 'image', 'therapy', 'hear', 'aid', 'product', 'laboratory', 'diagnostics', 'build', 'technology', 'financial', 'solution', 'solution', 'service', 'consumer', 'product', 'corporate', 'research', 'government', 'solution', 'information', 'communication', 'siemen', 'financial', 'solution', 'solution', 'service', 'siemen', 'usa', 'investor', 'relation', 'press', 'job', 'career', 'business', 'siemen', 'global', 'website', 'answer', 'america', 'renewable', 'energy', 'smart', 'grid', 'technology', 'medical', 'image', 'electronic', 'healthcare', 'record', 'green', 'build', 'commuter', 'rail', 'system', 'employee', 'siemen', 'unit', 'commit', 'answer', 'america', 'toughest', 'question', 'close', 'productfinder', 'layer', 'close', 'logo', 'layer', 'siemen', 'corporation', 'corporate', 'information', 'privacy', 'policy', 'term', 'digital', 'id']\n",
      "Sample 5: ['ad-symptom', 'ad-muscle', 'ad-weakness', 'ad-genetic', 'ad-disease', 'ad-symptom', 'ad-include', 'ad-search', 'ad-learn', 'page', 'found']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model, save_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Input, GlobalMaxPooling1D, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources (organizes linguistic data)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "    nltk.word_tokenize(\"example\") # This implicitly requires 'punkt'\n",
    "    WordNetLemmatizer().lemmatize(\"running\") # This implicitly requires 'wordnet'\n",
    "except LookupError as e:\n",
    "    print(f\"NLTK Resource not found: {e}\")\n",
    "    print(\"Downloading necessary NLTK resources...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"NLTK resources downloaded successfully.\")\n",
    "\n",
    "def load_farm_ads_data(text_file: str, vector_file: str):\n",
    "    # Dictionary to store index:value pairs\n",
    "    text_data = []\n",
    "    # Reads the data and separates the label and text\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            text = ' '.join(parts[1:])\n",
    "            text_data.append({'label': label, 'text': text})\n",
    "    \n",
    "    # Create pandas DataFrame where each item is a dictionary (key & value)\n",
    "    text_df = pd.DataFrame(text_data)\n",
    "    labels = text_df['label'].values\n",
    "    texts = text_df['text'].values\n",
    "    \n",
    "    vector_data = []\n",
    "    with open(vector_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            features = {}\n",
    "            for item in parts[1:]:\n",
    "                idx, val = item.split(':')\n",
    "                # '3:1' to idx = '3', val = '1'\n",
    "                features[int(idx)] = float(val)\n",
    "            vector_data.append(features)\n",
    "    \n",
    "    # Convert dictionaries into pandas DataFrame\n",
    "    vector_df = pd.DataFrame(vector_data).fillna(0)\n",
    "    \n",
    "    return texts, labels, vector_df\n",
    "\n",
    "def preprocess_text(text_series):\n",
    "    \"\"\"\n",
    "        Preprocess text by removing special characters, removing stopwords (\"the\", \"is\", \"and\", etc.),\n",
    "        and lemmatizing words (running, runs and ran to the base form \"run\".\n",
    "        \n",
    "        This reduces the dimensionality of text data and performance of downstram tasks liek text classificaiton or information retrieval\n",
    "    \"\"\"   \n",
    "    cleaned_texts = []\n",
    "    tokenized_texts = []\n",
    "    \n",
    "    for text in text_series:\n",
    "        tokens = word_tokenize(text)\n",
    "        # Store the tokenized version for Word2Vec\n",
    "        tokenized_texts.append(tokens)\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    # cleaned_texts for text classification and feature extration techiques like TF-IDF\n",
    "    # tokenized_text for token list used lated on word embeddings (Word2Vec) or RNN\n",
    "    return cleaned_texts, tokenized_texts\n",
    "\n",
    "def visualize_data(labels, text, cleaned_texts):\n",
    "    \"\"\"\n",
    "        Visualize data distributions and characteristics\n",
    "    \"\"\"\n",
    "    # Distribution of classes with -1 and 1\n",
    "    plt.figure(figsize=(8, 6)) #(width, height)\n",
    "    sns.countplot(x=labels)\n",
    "    plt.title('Distribution of Ad Classes')\n",
    "    plt.xlabel('Class (-1: Not Accepted, 1: Accepted)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('visualization/distributions.png')  \n",
    "    plt.close()\n",
    "    \n",
    "    all_words = [word for text in cleaned_texts for word in text.split()]\n",
    "    word_freq = pd.Series(all_words).value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    word_freq[:20].plot(kind='bar')\n",
    "    plt.title('Top 20 Most Common Words')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/word_frequency.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compare word frequencies between classes\n",
    "    accepted_words = [word for i, text in enumerate(cleaned_texts)\n",
    "                      for word in text.split() if labels[i] == 1]\n",
    "    rejected_words = [word for i, text in enumerate(cleaned_texts)\n",
    "                      for word in text.split() if labels[i] == -1]\n",
    "    \n",
    "    accepted_freq = pd.Series(accepted_words).value_counts()[:15]\n",
    "    rejected_freq = pd.Series(rejected_words).value_counts()[:15]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    accepted_freq.plot(kind='bar')\n",
    "    plt.title('Top Words in Accepted Ads')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    rejected_freq.plot(kind='bar')\n",
    "    plt.title('Top Words in Rejected Ads')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/class_word_frequency.png')\n",
    "    plt.close()\n",
    "\n",
    "# Build a deep CNN model with at least 10 layers\n",
    "def build_cnn_model(vocab_size, embedding_dim, embedding_matrix=None):\n",
    "    \"\"\"\n",
    "    It compiles the model with binary cross-entropy loss (How wrong are the predictions \"Yes, No\" Decisions) and the Adam optimizer (gradually improved by minimizing the errors).\n",
    "    \n",
    "    Learn patterns from text data to decide between two classes (positive vs negative).\n",
    "    Word embedding to turn words into numbers ->\n",
    "    Goes to several layers to detect important features ->\n",
    "    use normalization and dropout to learn better + avoid overfitting\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    # Layer 2: Spatial Dropout\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    # Layers 3-4: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 5: MaxPooling\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    # Layers 6-7: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 8: MaxPooling\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    # Layers 9-10: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 11: Global Max Pooling\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Layer 12: Dense + BatchNorm\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 13: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 14: Dense output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a LSTM model with at least 10 layers\n",
    "def build_lstm_model(vocab_size, embedding_dim, embedding_matrix=None):\n",
    "    \"\"\"    \n",
    "    This method creates a deep LSTM (Long Short-Term Memory) model that can learn patterns in sequences of words (like sentences), \n",
    "    especially useful for text classification. \n",
    "    It uses layers like LSTM (to capture sequence context), dropout (to prevent overfitting), \n",
    "    batch normalization (to stabilize learning), \n",
    "    and dense layers to make the final prediction between two classes (e.g., positive or negative).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    # Layer 2: Spatial Dropout\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    # Layer 3: LSTM\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    \n",
    "    # Layer 4: Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 5: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 6: LSTM\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # Layer 7: Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 8: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 9: Dense + Batch Normalization\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 10: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 11: Dense output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=10, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    This method trains a neural network model using training data, validates it during training to avoid overfitting (with early stopping), \n",
    "    and then evaluates its performance on test data using accuracy, classification report, confusion matrix, and ROC curve. \n",
    "    It also saves training history and evaluation plots (accuracy/loss, confusion matrix, and ROC curve) to files for later review.\n",
    "    \"\"\"\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Convert -1/1 back to 0/1 for evaluation if needed\n",
    "    if -1 in y_test:\n",
    "        y_test_01 = (y_test == 1).astype(int)\n",
    "    else:\n",
    "        y_test_01 = y_test\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test_01, y_pred)\n",
    "    report = classification_report(y_test_01, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test_01, y_pred)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization/{model_name}_training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Accepted', 'Accepted'],\n",
    "                yticklabels=['Not Accepted', 'Accepted'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization/{model_name}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_01, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'visualization/{model_name}_roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'roc_auc': roc_auc,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    text_file = \"farm-ads\"\n",
    "    vector_file = \"farm-ads-vect\"\n",
    "    \n",
    "    # Load data\n",
    "    texts, labels, vector_df = load_farm_ads_data(text_file, vector_file)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} text samples with labels: {np.unique(labels)}\")\n",
    "    \n",
    "    # Preprocess text data\n",
    "    cleaned_texts, tokenized_texts = preprocess_text(texts)\n",
    "    \n",
    "    # # Print the cleaned texts\n",
    "    # print(\"\\nCleaned Texts:\")\n",
    "    # for i, cleaned_text in enumerate(cleaned_texts[:5]):  # Print the first 5 \n",
    "    #     print(f\"Sample {i+1}: {cleaned_text}\")\n",
    "    # \n",
    "    # # Print the tokenized texts\n",
    "    # print(\"\\nTokenized Texts:\")\n",
    "    # for i, tokens in enumerate(tokenized_texts[:5]):  # Print the first 5 \n",
    "    #     print(f\"Sample {i+1}: {tokens}\")\n",
    "        \n",
    "    visualize_data(labels, texts, cleaned_texts)\n",
    "    \n",
    "    # Convert labels from -1/1 to 0/1 for binary classification\n",
    "    labels_01 = (labels == 1).astype(int)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cleaned_texts, labels_01, test_size=0.2, random_state=42, stratify=labels_01\n",
    "    )\n",
    "    \n",
    "    # Keep original labels for model evaluation\n",
    "    _, _, y_train_orig, y_test_orig = train_test_split(\n",
    "        cleaned_texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
    "    \n",
    "    # Prepare data for models\n",
    "    # Tokenize text for CNN and LSTM models\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # +1 to ensure index 0 for padding in sequence models\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # Pad sequences (makes sequences uniform in length for neural networks by adding 0 (padding) or cut off excess (truncation)\n",
    "    max_length = 100  # Can adjust text length analysis\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "    \n",
    "    # Create Word2Vec embeddings\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    # Create embedding matrix for pre-trained embeddings\n",
    "    embedding_dim = 100\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    \n",
    "    # Model 1: CNN with Word2Vec embeddings\n",
    "    print(\"\\nTraining CNN model with Word2Vec embeddings...\")\n",
    "    cnn_model = build_cnn_model(vocab_size, embedding_dim, embedding_matrix)\n",
    "    cnn_results = train_evaluate_model(\n",
    "        cnn_model, X_train_pad, y_train, X_test_pad, y_test_orig, \n",
    "        batch_size=32, epochs=15, model_name=\"CNN_Word2Vec\"\n",
    "    )\n",
    "    \n",
    "    # Model 2: LSTM with Word2Vec embeddings\n",
    "    print(\"\\nTraining LSTM model with Word2Vec embeddings...\")\n",
    "    lstm_model = build_lstm_model(vocab_size, embedding_dim, embedding_matrix)\n",
    "    lstm_results = train_evaluate_model(\n",
    "        lstm_model, X_train_pad, y_train, X_test_pad, y_test_orig, \n",
    "        batch_size=32, epochs=15, model_name=\"LSTM_Word2Vec\"\n",
    "    )\n",
    "    \n",
    "    # Compare all models\n",
    "    all_model_results = {\n",
    "        'CNN_Word2Vec': cnn_results,\n",
    "        'LSTM_Word2Vec': lstm_results\n",
    "    }\n",
    "        \n",
    "    # Saving Models\n",
    "    # 1. Save Word2Vec (vectors and vocabulary)\n",
    "    w2v_model.save(\"models/word2vec.model\")\n",
    "    np.save(\"models/embedding_matrix.npy\", embedding_matrix)\n",
    "    # 2. Save CNN (architecture 'layers', weights, optimizer state)\n",
    "    cnn_model.save(\"models/cnn_word2vec.h5\")\n",
    "    # 3. Save LSTM \n",
    "    lstm_model.save(\"models/lstm_word2vec.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a40c8a-b2ff-4464-9c8d-1d26731fa7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
