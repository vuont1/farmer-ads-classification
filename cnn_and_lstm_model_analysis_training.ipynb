{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-29T13:43:33.014713Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4143 text samples with labels: [-1  1]\n",
      "Training set size: 3314, Test set size: 829\n",
      "Vocabulary size: 39293\n",
      "Training Word2Vec model...\n",
      "\n",
      "Training CNN model with Word2Vec embeddings...\n",
      "Epoch 1/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.7021 - loss: 0.6471 - val_accuracy: 0.6109 - val_loss: 0.6651\n",
      "Epoch 2/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.8116 - loss: 0.4014 - val_accuracy: 0.8808 - val_loss: 0.3111\n",
      "Epoch 3/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.8576 - loss: 0.3411 - val_accuracy: 0.8839 - val_loss: 0.2706\n",
      "Epoch 4/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.8560 - loss: 0.3226 - val_accuracy: 0.8839 - val_loss: 0.2771\n",
      "Epoch 5/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.8820 - loss: 0.2884 - val_accuracy: 0.9050 - val_loss: 0.2483\n",
      "Epoch 6/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.9033 - loss: 0.2387 - val_accuracy: 0.8959 - val_loss: 0.2893\n",
      "Epoch 7/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 11ms/step - accuracy: 0.9158 - loss: 0.2137 - val_accuracy: 0.9020 - val_loss: 0.2359\n",
      "Epoch 8/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 12ms/step - accuracy: 0.9219 - loss: 0.1999 - val_accuracy: 0.8643 - val_loss: 0.3206\n",
      "Epoch 9/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 11ms/step - accuracy: 0.9163 - loss: 0.2184 - val_accuracy: 0.9080 - val_loss: 0.2589\n",
      "Epoch 10/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.9486 - loss: 0.1526 - val_accuracy: 0.8929 - val_loss: 0.2676\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step\n",
      "\n",
      "Training LSTM model with Word2Vec embeddings...\n",
      "Epoch 1/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 79ms/step - accuracy: 0.6123 - loss: 0.7593 - val_accuracy: 0.7572 - val_loss: 0.5447\n",
      "Epoch 2/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 82ms/step - accuracy: 0.7455 - loss: 0.5381 - val_accuracy: 0.8130 - val_loss: 0.4198\n",
      "Epoch 3/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 83ms/step - accuracy: 0.7811 - loss: 0.4789 - val_accuracy: 0.8416 - val_loss: 0.3577\n",
      "Epoch 4/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 76ms/step - accuracy: 0.8106 - loss: 0.4281 - val_accuracy: 0.8688 - val_loss: 0.3082\n",
      "Epoch 5/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 72ms/step - accuracy: 0.8414 - loss: 0.3763 - val_accuracy: 0.8733 - val_loss: 0.3048\n",
      "Epoch 6/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 83ms/step - accuracy: 0.8428 - loss: 0.3676 - val_accuracy: 0.8748 - val_loss: 0.3213\n",
      "Epoch 7/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 83ms/step - accuracy: 0.8562 - loss: 0.3399 - val_accuracy: 0.8643 - val_loss: 0.3299\n",
      "Epoch 8/15\n",
      "\u001B[1m83/83\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 82ms/step - accuracy: 0.8782 - loss: 0.2965 - val_accuracy: 0.8748 - val_loss: 0.3310\n",
      "\u001B[1m26/26\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model, save_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Input, GlobalMaxPooling1D, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources (organizes linguistic data)\n",
    "try:\n",
    "    nltk.word_tokenize(\"example\") # This implicitly requires 'punkt'\n",
    "    WordNetLemmatizer().lemmatize(\"running\") # This implicitly requires 'wordnet'\n",
    "except LookupError as e:\n",
    "    print(f\"NLTK Resource not found: {e}\")\n",
    "    print(\"Downloading necessary NLTK resources...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"NLTK resources downloaded successfully.\")\n",
    "\n",
    "def load_farm_ads_data(text_file: str, vector_file: str):\n",
    "    # Dictionary to store index:value pairs\n",
    "    text_data = []\n",
    "    # Reads the data and separates the label and text\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            text = ' '.join(parts[1:])\n",
    "            text_data.append({'label': label, 'text': text})\n",
    "    \n",
    "    # Create pandas DataFrame where each item is a dictionary (key & value)\n",
    "    text_df = pd.DataFrame(text_data)\n",
    "    labels = text_df['label'].values\n",
    "    texts = text_df['text'].values\n",
    "    \n",
    "    vector_data = []\n",
    "    with open(vector_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            features = {}\n",
    "            for item in parts[1:]:\n",
    "                idx, val = item.split(':')\n",
    "                # '3:1' to idx = '3', val = '1'\n",
    "                features[int(idx)] = float(val)\n",
    "            vector_data.append(features)\n",
    "    \n",
    "    # Convert dictionaries into pandas DataFrame\n",
    "    vector_df = pd.DataFrame(vector_data).fillna(0)\n",
    "    \n",
    "    return texts, labels, vector_df\n",
    "\n",
    "def preprocess_text(text_series):\n",
    "    \"\"\"      \n",
    "        This reduces the dimensionality of text data and performance of downstram tasks like text classificaiton or information retrieval\n",
    "    \"\"\"   \n",
    "    cleaned_texts = []\n",
    "    tokenized_texts = []\n",
    "    \n",
    "    for text in text_series:\n",
    "        tokens = word_tokenize(text)\n",
    "        # Store the tokenized version for Word2Vec\n",
    "        tokenized_texts.append(tokens)\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    # cleaned_texts for text classification and feature extration techiques like TF-IDF\n",
    "    # tokenized_text for token list used lated on word embeddings (Word2Vec) or RNN\n",
    "    return cleaned_texts, tokenized_texts\n",
    "\n",
    "def visualize_data(labels, texts, cleaned_texts):\n",
    "    \"\"\"\n",
    "        Visualize data distributions and characteristics\n",
    "    \"\"\"\n",
    "    # Distribution of classes with -1 and 1\n",
    "    plt.figure(figsize=(8, 6)) #(width, height)\n",
    "    sns.countplot(x=labels)\n",
    "    plt.title('Distribution of Ad Classes')\n",
    "    plt.xlabel('Class (-1: Not Accepted, 1: Accepted)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('visualization/distributions.png')  \n",
    "    plt.close()\n",
    "    \n",
    "    all_words = [word for text in cleaned_texts for word in text.split()]\n",
    "    word_freq = pd.Series(all_words).value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    word_freq[:20].plot(kind='bar')\n",
    "    plt.title('Top 20 Most Common Words')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/word_frequency.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compare word frequencies between classes\n",
    "    accepted_words = [word for i, text in enumerate(cleaned_texts)\n",
    "                      for word in text.split() if labels[i] == 1]\n",
    "    rejected_words = [word for i, text in enumerate(cleaned_texts)\n",
    "                      for word in text.split() if labels[i] == -1]\n",
    "    \n",
    "    accepted_freq = pd.Series(accepted_words).value_counts()[:15]\n",
    "    rejected_freq = pd.Series(rejected_words).value_counts()[:15]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    accepted_freq.plot(kind='bar')\n",
    "    plt.title('Top Words in Accepted Ads')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    rejected_freq.plot(kind='bar')\n",
    "    plt.title('Top Words in Rejected Ads')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/class_word_frequency.png')\n",
    "    plt.close()\n",
    "\n",
    "# Build a deep CNN model with at least 10 layers\n",
    "def build_cnn_model(vocab_size, embedding_dim, embedding_matrix=None):\n",
    "    \"\"\"\n",
    "    It compiles the model with binary cross-entropy loss (How wrong are the predictions \"Yes, No\" Decisions) and the Adam optimizer (gradually improved by minimizing the errors).\n",
    "    \n",
    "    Learn patterns from text data to decide between two classes (positive vs negative).\n",
    "    Word embedding to turn words into numbers ->\n",
    "    Goes to several layers to detect important features ->\n",
    "    use normalization and dropout to learn better + avoid overfitting\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    # Layer 2: Spatial Dropout\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    # Layers 3-4: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 5: MaxPooling\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    # Layers 6-7: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 8: MaxPooling\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    # Layers 9-10: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 11: Global Max Pooling\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Layer 12: Dense + BatchNorm\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 13: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 14: Dense output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a LSTM model with at least 10 layers\n",
    "def build_lstm_model(vocab_size, embedding_dim, embedding_matrix=None):\n",
    "    \"\"\"    \n",
    "    This method creates a deep LSTM (Long Short-Term Memory) model that can learn patterns in sequences of words (like sentences), \n",
    "    especially useful for text classification. \n",
    "    It uses layers like LSTM (to capture sequence context), dropout (to prevent overfitting), \n",
    "    batch normalization (to stabilize learning), \n",
    "    and dense layers to make the final prediction between two classes (e.g., positive or negative).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    # Layer 2: Spatial Dropout\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    # Layer 3: LSTM\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    \n",
    "    # Layer 4: Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 5: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 6: LSTM\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # Layer 7: Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 8: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 9: Dense + Batch Normalization\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 10: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 11: Dense output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=10, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    This method trains a neural network model using training data, validates it during training to avoid overfitting (with early stopping), \n",
    "    and then evaluates its performance on test data using accuracy, classification report, confusion matrix, and ROC curve. \n",
    "    It also saves training history and evaluation plots (accuracy/loss, confusion matrix, and ROC curve) to files for later review.\n",
    "    \"\"\"\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Convert -1/1 back to 0/1 for evaluation if needed\n",
    "    if -1 in y_test:\n",
    "        y_test_01 = (y_test == 1).astype(int)\n",
    "    else:\n",
    "        y_test_01 = y_test\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test_01, y_pred)\n",
    "    report = classification_report(y_test_01, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test_01, y_pred)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization/{model_name}_training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Accepted', 'Accepted'],\n",
    "                yticklabels=['Not Accepted', 'Accepted'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization/{model_name}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_01, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'visualization/{model_name}_roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'roc_auc': roc_auc,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    text_file = \"farm-ads\"\n",
    "    vector_file = \"farm-ads-vect\"\n",
    "    \n",
    "    # Load data\n",
    "    texts, labels, vector_df = load_farm_ads_data(text_file, vector_file)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} text samples with labels: {np.unique(labels)}\")\n",
    "    \n",
    "    # Preprocess text data\n",
    "    cleaned_texts, tokenized_texts = preprocess_text(texts)\n",
    "    \n",
    "    # # Print the cleaned texts\n",
    "    # print(\"\\nCleaned Texts:\")\n",
    "    # for i, cleaned_text in enumerate(cleaned_texts[:5]):  # Print the first 5 \n",
    "    #     print(f\"Sample {i+1}: {cleaned_text}\")\n",
    "    # \n",
    "    # # Print the tokenized texts\n",
    "    # print(\"\\nTokenized Texts:\")\n",
    "    # for i, tokens in enumerate(tokenized_texts[:5]):  # Print the first 5 \n",
    "    #     print(f\"Sample {i+1}: {tokens}\")\n",
    "        \n",
    "    visualize_data(labels, texts, cleaned_texts)\n",
    "    \n",
    "    # Convert labels from -1/1 to 0/1 for binary classification\n",
    "    labels_01 = (labels == 1).astype(int)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cleaned_texts, labels_01, test_size=0.2, random_state=42, stratify=labels_01\n",
    "    )\n",
    "    \n",
    "    # Keep original labels for model evaluation\n",
    "    _, _, y_train_orig, y_test_orig = train_test_split(\n",
    "        cleaned_texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
    "    \n",
    "    # Prepare data for models\n",
    "    # Tokenize text for CNN and LSTM models\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # +1 to ensure index 0 for padding in sequence models\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # Pad sequences (makes sequences uniform in length for neural networks by adding 0 (padding) or cut off excess (truncation)\n",
    "    max_length = 100  # Can adjust text length analysis\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "    \n",
    "    # Create Word2Vec embeddings\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    # Create embedding matrix for pre-trained embeddings\n",
    "    embedding_dim = 100\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    \n",
    "    # Model 1: CNN with Word2Vec embeddings\n",
    "    print(\"\\nTraining CNN model with Word2Vec embeddings...\")\n",
    "    cnn_model = build_cnn_model(vocab_size, embedding_dim, embedding_matrix)\n",
    "    cnn_results = train_evaluate_model(\n",
    "        cnn_model, X_train_pad, y_train, X_test_pad, y_test_orig, \n",
    "        batch_size=32, epochs=15, model_name=\"CNN_Word2Vec\"\n",
    "    )\n",
    "    \n",
    "    # Model 2: LSTM with Word2Vec embeddings\n",
    "    print(\"\\nTraining LSTM model with Word2Vec embeddings...\")\n",
    "    lstm_model = build_lstm_model(vocab_size, embedding_dim, embedding_matrix)\n",
    "    lstm_results = train_evaluate_model(\n",
    "        lstm_model, X_train_pad, y_train, X_test_pad, y_test_orig, \n",
    "        batch_size=32, epochs=15, model_name=\"LSTM_Word2Vec\"\n",
    "    )\n",
    "    \n",
    "    # Compare all models\n",
    "    all_model_results = {\n",
    "        'CNN_Word2Vec': cnn_results,\n",
    "        'LSTM_Word2Vec': lstm_results\n",
    "    }\n",
    "        \n",
    "    # Saving Models\n",
    "    # 1. Save Word2Vec (vectors and vocabulary)\n",
    "    w2v_model.save(\"models/word2vec.model\")\n",
    "    np.save(\"models/embedding_matrix.npy\", embedding_matrix)\n",
    "    # 2. Save CNN (architecture 'layers', weights, optimizer state)\n",
    "    cnn_model.save(\"models/cnn_word2vec.keras\")\n",
    "    # 3. Save LSTM \n",
    "    lstm_model.save(\"models/lstm_word2vec.keras\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a40c8a-b2ff-4464-9c8d-1d26731fa7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
