{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T08:43:28.662518Z",
     "start_time": "2025-05-17T08:43:23.144335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 text samples with labels:\n",
      "Label: 1\tText: ad-jerry ad-bruckheimer ad-chase ad-premier ad-sep...\n",
      "Label: -1\tText: ad-rheumatoid ad-arthritis ad-expert ad-tip ad-inf...\n",
      "Label: -1\tText: ad-rheumatologist ad-anju ad-varghese ad-yonker ad...\n",
      "Label: -1\tText: ad-siemen ad-water ad-remediation ad-water ad-scar...\n",
      "Label: -1\tText: ad-symptom ad-muscle ad-weakness ad-genetic ad-dis...\n",
      "\n",
      "Vector data shape: (4143, 54877)\n",
      "First 5 rows of vector data (non-zero features only):\n",
      "0    {1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1....\n",
      "1    {10: 1.0, 11: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, ...\n",
      "2    {29: 1.0, 31: 1.0, 35: 1.0, 101: 1.0, 131: 1.0...\n",
      "3    {34: 1.0, 35: 1.0, 36: 1.0, 44: 1.0, 54: 1.0, ...\n",
      "4    {8: 1.0, 9: 1.0, 429: 1.0, 430: 1.0, 431: 1.0,...\n",
      "dtype: object\n",
      "\n",
      "=== Data Shapes ===\n",
      "Word2Vec Train Sequences: (3314, 128)\n",
      "Word2Vec Test Sequences: (829, 128)\n",
      "Vector Features Train: (3314, 54877)\n",
      "Vector Features Test: (829, 54877)\n",
      "Labels Train: (3314, 1)\n",
      "Labels Test: (829, 1)\n",
      "\n",
      "=== Word2Vec Model Info ===\n",
      "Vocabulary size: 54877\n",
      "Embedding dimension: 100\n",
      "Sample words and their vectors:\n",
      "'list': [-1.5129446   0.36026213 -0.7893673   1.7757297  -3.9405403 ]...\n",
      "'product': [-2.2874932  -1.2048829   0.66811746  1.7529045  -1.5942047 ]...\n",
      "'com': [ 0.23906775  0.3684321  -3.586315    2.5866446   1.5665727 ]...\n",
      "\n",
      "=== Sample Text Sequences ===\n",
      "First training sample (word indices):\n",
      "[ 1950 28218   978  5548    38    33   420    72   155   420    72   420\n",
      "    72   155   155   420    72   438   632    39 44198     2   155  3277\n",
      "    48    33   420    72    24    48    33   420    72   420  2101  2502\n",
      "   136    97  4724    39 44213     2   420    72   420    72 29819   420\n",
      "    72    38  1793   232   420    72   420    72   420    72    24    41\n",
      "   420    72 44212    39 26481     2   420    72  2134   241  1467    11\n",
      "   892  1089    11   241    13  2129   142  1000   395    39  5886     2\n",
      "   430   420    72   430   420    72  1008   734  1808    92   395     3\n",
      " 16076     2 44211   232   208    57  1524    54   387    54  2383    12\n",
      "  3410    29   152    12    68    90   103    31  7729  7586  1916  7743\n",
      "  7386   507   776  2406  3004  7621   110   178]\n",
      "\n",
      "Corresponding original text:\n",
      "ad-jerry ad-bruckheimer ad-chase ad-premier ad-sept ad-th ad-clip ad-bruckheimer ad-chase page found...\n",
      "\n",
      "=== Vector Feature Samples ===\n",
      "First training sample (non-zero features only):\n",
      "54877 non-zero features out of 54877\n",
      "[(0, -0.015537994118497435), (1, -0.015537994118497435), (2, -0.026919095102908273), (3, -0.04113947592872961), (4, -0.021976695083223287)]\n",
      "\n",
      "=== Label Distribution ===\n",
      "Training set:\n",
      "{-1: 1546, 1: 1768}\n",
      "\n",
      "Test set:\n",
      "{-1: 387, 1: 442}\n",
      "\n",
      "=== Embedding Matrix ===\n",
      "Shape: (54878, 100)\n",
      "Sample embedding (first word):\n",
      "[-2.28749323 -1.20488286  0.66811746  1.75290453 -1.59420466 -0.40446499\n",
      "  0.20238468  1.21093786 -1.74364674  0.68955189]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Loading the data\n",
    "def load_farm_ads_data(text_file: str, vector_file: str):\n",
    "    \"\"\"Load and parse the farm ads data files\"\"\"\n",
    "    # Dictionary to store index:value pairs\n",
    "    text_data = []\n",
    "    # Reads the data and seperates the label and text\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            text = ' '.join(parts[1:])\n",
    "            text_data.append({'label': label, 'text': text})\n",
    "    \n",
    "    # Create pandas DataFrame where each item is a dictionary (key & value)\n",
    "    text_df = pd.DataFrame(text_data)\n",
    "    labels = text_df['label'].values\n",
    "    texts = text_df['text'].values\n",
    "    \n",
    "    vector_data = []\n",
    "    with open(vector_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            features = {}\n",
    "            for item in parts[1:]:\n",
    "                idx, val = item.split(':')\n",
    "                # '3:1' to idx = '3', val = '1'\n",
    "                features[int(idx)] = float(val)\n",
    "            vector_data.append(features)\n",
    "    \n",
    "    # Convert dictionaries into pandas DataFrame\n",
    "    vector_df = pd.DataFrame(vector_data).fillna(0)\n",
    "    \n",
    "    return texts, labels, vector_df\n",
    "\n",
    "def preprocess_data(texts, labels, vector_features, test_size=0.2, random_state=123, w2v_size=100, w2v_window=5, w2v_min_count=1):\n",
    "    \"\"\"\n",
    "    Preprocess data with:\n",
    "        - Word2Vec embeddings\n",
    "        - BERT tokenization\n",
    "        - Vector feature normalization\n",
    "    \"\"\"\n",
    "    # Convert labels to numpy array (-1, 1 format)\n",
    "    y = np.array(labels).reshape(-1, 1)\n",
    "    \n",
    "    # Word2Vec Embeddings - Tokenize texts for Word2Vec\n",
    "    tokenized_texts = [text.split() for text in texts]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=w2v_size,\n",
    "        window=w2v_window,\n",
    "        min_count=w2v_min_count,\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(w2v_model.wv.key_to_index) + 1, w2v_size))\n",
    "    for word, idx in w2v_model.wv.key_to_index.items():\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    \n",
    "    # Convert texts to sequences of word indices\n",
    "    word_index = w2v_model.wv.key_to_index\n",
    "    sequences = [[word_index[word] for word in text.split() if word in word_index] for text in texts]\n",
    "    X_text_seq = pad_sequences(sequences, maxlen=128)\n",
    "    \n",
    "    # Bert Tokenization\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    bert_inputs = bert_tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1000,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    \n",
    "    # Vector Feature Normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_vector = scaler.fit_transform(vector_features)\n",
    "    \n",
    "    # Create consistent split indices\n",
    "    indices = np.arange(len(y))\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        indices, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Splits to all representations\n",
    "    X_train_seq, X_test_seq = X_text_seq[train_idx], X_text_seq[test_idx]\n",
    "    X_train_vec, X_test_vec = X_vector[train_idx], X_vector[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Split Bert inputs\n",
    "    bert_train = {\n",
    "        'input_ids': tf.gather(bert_inputs['input_ids'], train_idx),\n",
    "        'attention_mask': tf.gather(bert_inputs['attention_mask'], train_idx)\n",
    "    }\n",
    "    bert_test = {\n",
    "        'input_ids': tf.gather(bert_inputs['input_ids'], test_idx),\n",
    "        'attention_mask': tf.gather(bert_inputs['attention_mask'], test_idx)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'word_embeddings': (X_train_seq, X_test_seq, X_train_vec, X_test_vec, y_train, y_test),\n",
    "        'bert': (bert_train, bert_test, X_train_vec, X_test_vec, y_train, y_test),\n",
    "        'w2v_model': w2v_model,\n",
    "        'embedding_matrix': embedding_matrix,\n",
    "        'bert_tokenizer': bert_tokenizer,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File paths\n",
    "    text_file = \"farm-ads\"\n",
    "    vector_file = \"farm-ads-vect\"\n",
    "    \n",
    "    # Load data\n",
    "    texts, labels, vector_data = load_farm_ads_data(text_file, vector_file)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nFirst 5 text samples with labels:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Label: {labels[i]}\\tText: {texts[i][:50]}...\")\n",
    "    \n",
    "    print(\"\\nVector data shape:\", vector_data.shape)\n",
    "    print(\"First 5 rows of vector data (non-zero features only):\")\n",
    "    # Get column wise + dictionary of non-zero features\n",
    "    print(vector_data.iloc[:5].apply(lambda x: x[x != 0].to_dict(), axis=1))\n",
    "    \n",
    "    # ===================\n",
    "    # Preprocess with original (-1,1) labels\n",
    "    processed = preprocess_data(\n",
    "        texts, \n",
    "        labels, \n",
    "        vector_data.values,\n",
    "        w2v_size=100,  # Embedding dimension\n",
    "        w2v_window=5,  # Context window size\n",
    "        w2v_min_count=1  # Minimum word frequency\n",
    "    )\n",
    "    \n",
    "    # 1. Print basic shapes and info\n",
    "    print(\"\\n=== Data Shapes ===\")\n",
    "    print(f\"Word2Vec Train Sequences: {processed['word_embeddings'][0].shape}\")\n",
    "    print(f\"Word2Vec Test Sequences: {processed['word_embeddings'][1].shape}\")\n",
    "    print(f\"Vector Features Train: {processed['word_embeddings'][2].shape}\")\n",
    "    print(f\"Vector Features Test: {processed['word_embeddings'][3].shape}\")\n",
    "    print(f\"Labels Train: {processed['word_embeddings'][4].shape}\")\n",
    "    print(f\"Labels Test: {processed['word_embeddings'][5].shape}\")\n",
    "    \n",
    "    # 2. Print Word2Vec model info\n",
    "    print(\"\\n=== Word2Vec Model Info ===\")\n",
    "    print(f\"Vocabulary size: {len(processed['w2v_model'].wv)}\")\n",
    "    print(f\"Embedding dimension: {processed['w2v_model'].vector_size}\")\n",
    "    print(\"Sample words and their vectors:\")\n",
    "    for i, word in enumerate(processed['w2v_model'].wv.index_to_key[:3]):  # First 3 words\n",
    "        print(f\"'{word}': {processed['w2v_model'].wv[word][:5]}...\")  # First 5 dimensions\n",
    "    \n",
    "    # 3. Print sample sequences\n",
    "    print(\"\\n=== Sample Text Sequences ===\")\n",
    "    print(\"First training sample (word indices):\")\n",
    "    print(processed['word_embeddings'][0][0])\n",
    "    print(\"\\nCorresponding original text:\")\n",
    "    print(texts[0][:100] + \"...\")  # First 100 chars\n",
    "    \n",
    "    # 4. Print vector features\n",
    "    print(\"\\n=== Vector Feature Samples ===\")\n",
    "    print(\"First training sample (non-zero features only):\")\n",
    "    first_sample = processed['word_embeddings'][2][0]  # First sample's vector features\n",
    "    non_zero = {i: val for i, val in enumerate(first_sample) if val != 0}\n",
    "    print(f\"{len(non_zero)} non-zero features out of {len(first_sample)}\")\n",
    "    print(list(non_zero.items())[:5])  # First 5 non-zero features\n",
    "    \n",
    "    # 5. Print label distribution\n",
    "    print(\"\\n=== Label Distribution ===\")\n",
    "    print(\"Training set:\")\n",
    "    unique, counts = np.unique(processed['word_embeddings'][4], return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"\\nTest set:\")\n",
    "    unique, counts = np.unique(processed['word_embeddings'][5], return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    \n",
    "    # 6. Print embedding matrix info\n",
    "    print(\"\\n=== Embedding Matrix ===\")\n",
    "    print(f\"Shape: {processed['embedding_matrix'].shape}\")\n",
    "    print(\"Sample embedding (first word):\")\n",
    "    print(processed['embedding_matrix'][1][:10])  # First 10 dimensions of 2nd row (index 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1dfdf-e408-46ee-b114-b6efa58e6a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
