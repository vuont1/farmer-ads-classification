{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-30T13:12:06.433645Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davidvuong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/davidvuong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4143 text samples with labels: [-1  1]\n",
      "Training set size: 3314, Test set size: 829\n",
      "Vocabulary size: 39293\n",
      "Training Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN model with Word2Vec embeddings...\n",
      "Epoch 1/15\n",
      "83/83 [==============================] - 2s 11ms/step - loss: 0.5993 - accuracy: 0.7276 - val_loss: 0.3881 - val_accuracy: 0.8446\n",
      "Epoch 2/15\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 0.4032 - accuracy: 0.8178 - val_loss: 0.3386 - val_accuracy: 0.8371\n",
      "Epoch 3/15\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 0.3554 - accuracy: 0.8438 - val_loss: 0.2566 - val_accuracy: 0.8989\n",
      "Epoch 4/15\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.3038 - accuracy: 0.8744 - val_loss: 0.2959 - val_accuracy: 0.8612\n",
      "Epoch 5/15\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.2905 - accuracy: 0.8789 - val_loss: 0.2956 - val_accuracy: 0.8824\n",
      "Epoch 6/15\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.2505 - accuracy: 0.8966 - val_loss: 0.2564 - val_accuracy: 0.8974\n",
      "Epoch 7/15\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 0.2250 - accuracy: 0.9098 - val_loss: 0.3066 - val_accuracy: 0.8914\n",
      "Epoch 8/15\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.2057 - accuracy: 0.9185 - val_loss: 0.3339 - val_accuracy: 0.8974\n",
      "Epoch 9/15\n",
      "83/83 [==============================] - 1s 13ms/step - loss: 0.1786 - accuracy: 0.9302 - val_loss: 0.3028 - val_accuracy: 0.8974\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "\n",
      "Training LSTM model with Word2Vec embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "83/83 [==============================] - 14s 150ms/step - loss: 0.7085 - accuracy: 0.6416 - val_loss: 0.5311 - val_accuracy: 0.7753\n",
      "Epoch 2/15\n",
      "83/83 [==============================] - 11s 136ms/step - loss: 0.5523 - accuracy: 0.7356 - val_loss: 0.4230 - val_accuracy: 0.8100\n",
      "Epoch 3/15\n",
      "83/83 [==============================] - 12s 146ms/step - loss: 0.4688 - accuracy: 0.7831 - val_loss: 0.3494 - val_accuracy: 0.8371\n",
      "Epoch 4/15\n",
      "83/83 [==============================] - 12s 139ms/step - loss: 0.4358 - accuracy: 0.8159 - val_loss: 0.3456 - val_accuracy: 0.8416\n",
      "Epoch 5/15\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 0.3950 - accuracy: 0.8336 - val_loss: 0.3123 - val_accuracy: 0.8748\n",
      "Epoch 6/15\n",
      "83/83 [==============================] - 13s 154ms/step - loss: 0.3756 - accuracy: 0.8412 - val_loss: 0.4784 - val_accuracy: 0.8446\n",
      "Epoch 7/15\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 0.3663 - accuracy: 0.8438 - val_loss: 0.3080 - val_accuracy: 0.8703\n",
      "Epoch 8/15\n",
      "83/83 [==============================] - 13s 153ms/step - loss: 0.3228 - accuracy: 0.8638 - val_loss: 0.3016 - val_accuracy: 0.8778\n",
      "Epoch 9/15\n",
      "83/83 [==============================] - 12s 150ms/step - loss: 0.3112 - accuracy: 0.8680 - val_loss: 0.2578 - val_accuracy: 0.8869\n",
      "Epoch 10/15\n",
      "83/83 [==============================] - 13s 162ms/step - loss: 0.2907 - accuracy: 0.8804 - val_loss: 0.2563 - val_accuracy: 0.8974\n",
      "Epoch 11/15\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 0.2716 - accuracy: 0.8865 - val_loss: 0.2801 - val_accuracy: 0.8899\n",
      "Epoch 12/15\n",
      "83/83 [==============================] - 14s 169ms/step - loss: 0.2482 - accuracy: 0.8978 - val_loss: 0.2789 - val_accuracy: 0.8944\n",
      "Epoch 13/15\n",
      "83/83 [==============================] - 13s 158ms/step - loss: 0.2587 - accuracy: 0.8917 - val_loss: 0.2492 - val_accuracy: 0.8929\n",
      "Epoch 14/15\n",
      "83/83 [==============================] - 9s 114ms/step - loss: 0.2481 - accuracy: 0.9038 - val_loss: 0.3033 - val_accuracy: 0.8778\n",
      "Epoch 15/15\n",
      "83/83 [==============================] - 13s 155ms/step - loss: 0.2255 - accuracy: 0.9110 - val_loss: 0.2664 - val_accuracy: 0.8989\n",
      "26/26 [==============================] - 2s 79ms/step\n",
      "\n",
      "Training BERT-based model (Transfer Learning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "166/166 [==============================] - 206s 1s/step - loss: 0.9568 - accuracy: 0.4960 - val_loss: 0.7556 - val_accuracy: 0.4525\n",
      "Epoch 2/5\n",
      "166/166 [==============================] - 202s 1s/step - loss: 0.8985 - accuracy: 0.5153 - val_loss: 0.7985 - val_accuracy: 0.4706\n",
      "Epoch 3/5\n",
      "166/166 [==============================] - 205s 1s/step - loss: 0.8944 - accuracy: 0.5160 - val_loss: 0.7892 - val_accuracy: 0.4842\n",
      "Epoch 4/5\n",
      "166/166 [==============================] - 207s 1s/step - loss: 0.8880 - accuracy: 0.5281 - val_loss: 0.7458 - val_accuracy: 0.4751\n",
      "Epoch 5/5\n",
      "166/166 [==============================] - 209s 1s/step - loss: 0.8290 - accuracy: 0.5372 - val_loss: 0.6980 - val_accuracy: 0.5053\n",
      "26/26 [==============================] - 36s 1s/step\n",
      "\n",
      "Model Comparison:\n",
      "           Model  Accuracy   ROC AUC\n",
      "0   CNN_Word2Vec  0.867310  0.936769\n",
      "1  LSTM_Word2Vec  0.856454  0.943959\n",
      "2  BERT_Transfer  0.507841  0.644411\n",
      "\n",
      "CNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85       387\n",
      "           1       0.84      0.93      0.88       442\n",
      "\n",
      "    accuracy                           0.87       829\n",
      "   macro avg       0.87      0.86      0.87       829\n",
      "weighted avg       0.87      0.87      0.87       829\n",
      "\n",
      "\n",
      "LSTM Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       387\n",
      "           1       0.85      0.89      0.87       442\n",
      "\n",
      "    accuracy                           0.86       829\n",
      "   macro avg       0.86      0.85      0.86       829\n",
      "weighted avg       0.86      0.86      0.86       829\n",
      "\n",
      "\n",
      "BERT Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.93      0.64       387\n",
      "           1       0.70      0.14      0.23       442\n",
      "\n",
      "    accuracy                           0.51       829\n",
      "   macro avg       0.59      0.53      0.43       829\n",
      "weighted avg       0.60      0.51      0.42       829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_env/lib/python3.11/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam \n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import nltk\n",
    "# Download necessary NLTK resources\n",
    "#NLTK (Natural Language Toolkit) is a Python library for text processing tasks like tokenization, part-of-speech tagging, and sentiment analysis.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "def load_farm_ads_data(text_file: str, vector_file: str):\n",
    "    # Dictionary to store index:value pairs\n",
    "    text_data = []\n",
    "    # Reads the data and separates the label and text\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            text = ' '.join(parts[1:])\n",
    "            text_data.append({'label': label, 'text': text})\n",
    "    \n",
    "    # Create pandas DataFrame where each item is a dictionary (key & value)\n",
    "    text_df = pd.DataFrame(text_data)\n",
    "    labels = text_df['label'].values\n",
    "    texts = text_df['text'].values\n",
    "    \n",
    "    vector_data = []\n",
    "    with open(vector_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:  # skip empty lines\n",
    "                continue\n",
    "            label = int(parts[0])\n",
    "            features = {}\n",
    "            for item in parts[1:]:\n",
    "                idx, val = item.split(':')\n",
    "                # '3:1' to idx = '3', val = '1'\n",
    "                features[int(idx)] = float(val)\n",
    "            vector_data.append(features)\n",
    "    \n",
    "    # Convert dictionaries into pandas DataFrame\n",
    "    vector_df = pd.DataFrame(vector_data).fillna(0)\n",
    "    \n",
    "    return texts, labels, vector_df\n",
    "\n",
    "def preprocess_text(text_series):\n",
    "    \"\"\"      \n",
    "        This reduces the dimensionality of text data and performance of downstram tasks like text classificaiton or information retrieval\n",
    "    \"\"\"   \n",
    "    cleaned_texts = []\n",
    "    tokenized_texts = []\n",
    "    \n",
    "    for text in text_series:\n",
    "        tokens = word_tokenize(text)\n",
    "        # Store the tokenized version for Word2Vec\n",
    "        tokenized_texts.append(tokens)\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    # cleaned_texts for text classification and feature extration techiques like TF-IDF\n",
    "    # tokenized_text for token list used lated on word embeddings (Word2Vec) or RNN\n",
    "    return cleaned_texts, tokenized_texts\n",
    "\n",
    "def visualize_data(labels, texts, cleaned_texts):\n",
    "    \"\"\"\n",
    "        Visualize data distributions and characteristics\n",
    "    \"\"\"\n",
    "    # Distribution of classes with -1 and 1\n",
    "    plt.figure(figsize=(8, 6)) #(width, height)\n",
    "    sns.countplot(x=labels)\n",
    "    plt.title('Distribution of Ad Classes')\n",
    "    plt.xlabel('Class (-1: Not Accepted, 1: Accepted)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('visualization/distributions.png')  \n",
    "    plt.close()\n",
    "    \n",
    "    all_words = [word for text in cleaned_texts for word in text.split()]\n",
    "    word_freq = pd.Series(all_words).value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    word_freq[:20].plot(kind='bar')\n",
    "    plt.title('Top 20 Most Common Words')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/word_frequency.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compare word frequencies between classes\n",
    "    accepted_words = [word for i, text in enumerate(cleaned_texts)\n",
    "                      for word in text.split() if labels[i] == 1]\n",
    "    rejected_words = [word for i, text in enumerate(cleaned_texts)\n",
    "                      for word in text.split() if labels[i] == -1]\n",
    "    \n",
    "    accepted_freq = pd.Series(accepted_words).value_counts()[:15]\n",
    "    rejected_freq = pd.Series(rejected_words).value_counts()[:15]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    accepted_freq.plot(kind='bar')\n",
    "    plt.title('Top Words in Accepted Ads')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    rejected_freq.plot(kind='bar')\n",
    "    plt.title('Top Words in Rejected Ads')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/class_word_frequency.png')\n",
    "    plt.close()\n",
    "\n",
    "# Build a deep CNN model with at least 10 layers\n",
    "def build_cnn_model(vocab_size, embedding_dim, embedding_matrix=None):\n",
    "    \"\"\"\n",
    "    It compiles the model with binary cross-entropy loss (How wrong are the predictions \"Yes, No\" Decisions) and the Adam optimizer (gradually improved by minimizing the errors).\n",
    "    \n",
    "    Learn patterns from text data to decide between two classes (positive vs negative).\n",
    "    Word embedding to turn words into numbers ->\n",
    "    Goes to several layers to detect important features ->\n",
    "    use normalization and dropout to learn better + avoid overfitting\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    # Layer 2: Spatial Dropout\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    # Layers 3-4: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 5: MaxPooling\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    # Layers 6-7: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 8: MaxPooling\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    # Layers 9-10: Conv1D + BatchNorm\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 11: Global Max Pooling\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Layer 12: Dense + BatchNorm\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 13: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 14: Dense output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a LSTM model with at least 10 layers\n",
    "def build_lstm_model(vocab_size, embedding_dim, embedding_matrix=None):\n",
    "    \"\"\"    \n",
    "    This method creates a deep LSTM (Long Short-Term Memory) model that can learn patterns in sequences of words (like sentences), \n",
    "    especially useful for text classification. \n",
    "    It uses layers like LSTM (to capture sequence context), dropout (to prevent overfitting), \n",
    "    batch normalization (to stabilize learning), \n",
    "    and dense layers to make the final prediction between two classes (e.g., positive or negative).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Embedding layer\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    # Layer 2: Spatial Dropout\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    # Layer 3: LSTM\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    \n",
    "    # Layer 4: Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 5: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 6: LSTM\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    \n",
    "    # Layer 7: Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 8: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 9: Dense + Batch Normalization\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Layer 10: Dropout\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Layer 11: Dense output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a BERT-based transfer learning model\n",
    "def build_bert_model(max_length):\n",
    "    \"\"\"  \n",
    "       This function creates a neural network for text classification.\n",
    "        \n",
    "       It uses a pre-trained BERT model as its base, adds several dense layers\n",
    "       with batch normalization and dropout for further processing,\n",
    "       and then compiles the entire model for binary classification.\n",
    "    \"\"\"\n",
    "    # Import BERT model\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Define input layers\n",
    "    input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Get BERT embeddings - use the model directly with inputs\n",
    "    bert_outputs = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    \n",
    "    # Use pooled output for classification\n",
    "    pooled_output = bert_outputs.pooler_output\n",
    "    \n",
    "    # Add dropout\n",
    "    x = Dropout(0.3)(pooled_output)\n",
    "    \n",
    "    # Hidden layers\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Define model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    # Freeze BERT layers\n",
    "    bert_model.trainable = False\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=5e-5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=10, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    This method trains a neural network model using training data, validates it during training to avoid overfitting (with early stopping), \n",
    "    and then evaluates its performance on test data using accuracy, classification report, confusion matrix, and ROC curve. \n",
    "    It also saves training history and evaluation plots (accuracy/loss, confusion matrix, and ROC curve) to files for later review.\n",
    "    \"\"\"\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Convert -1/1 back to 0/1 for evaluation if needed\n",
    "    if -1 in y_test:\n",
    "        y_test_01 = (y_test == 1).astype(int)\n",
    "    else:\n",
    "        y_test_01 = y_test\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test_01, y_pred)\n",
    "    report = classification_report(y_test_01, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test_01, y_pred)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization/{model_name}_training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Accepted', 'Accepted'],\n",
    "                yticklabels=['Not Accepted', 'Accepted'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualization/{model_name}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_01, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'visualization/{model_name}_roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'roc_auc': roc_auc,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_models(model_results):\n",
    "    \"\"\"\n",
    "    Compare multiple models and visualize their performance\n",
    "    \"\"\"\n",
    "    # Extract metrics for comparison\n",
    "    models = list(model_results.keys())\n",
    "    accuracies = [model_results[model]['accuracy'] for model in models]\n",
    "    roc_aucs = [model_results[model]['roc_auc'] for model in models]\n",
    "    \n",
    "    # Prepare data for bar plot\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': models + models,\n",
    "        'Metric': ['Accuracy'] * len(models) + ['ROC AUC'] * len(models),\n",
    "        'Value': accuracies + roc_aucs\n",
    "    })\n",
    "    \n",
    "    # Create comparison bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Model', y='Value', hue='Metric', data=metrics_df)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_table = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'Accuracy': accuracies,\n",
    "        'ROC AUC': roc_aucs\n",
    "    })\n",
    "    \n",
    "    return comparison_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    text_file = \"farm-ads\"\n",
    "    vector_file = \"farm-ads-vect\"\n",
    "    \n",
    "    # Load data\n",
    "    texts, labels, vector_df = load_farm_ads_data(text_file, vector_file)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} text samples with labels: {np.unique(labels)}\")\n",
    "    \n",
    "    # Preprocess text data\n",
    "    cleaned_texts, tokenized_texts = preprocess_text(texts)\n",
    "        \n",
    "    visualize_data(labels, texts, cleaned_texts)\n",
    "    \n",
    "    # Convert labels from -1/1 to 0/1 for binary classification\n",
    "    labels_01 = (labels == 1).astype(int)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cleaned_texts, labels_01, test_size=0.2, random_state=42, stratify=labels_01\n",
    "    )\n",
    "    \n",
    "    # Keep original labels for model evaluation\n",
    "    _, _, y_train_orig, y_test_orig = train_test_split(\n",
    "        cleaned_texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
    "    \n",
    "    # Prepare data for models\n",
    "    # Tokenize text for CNN and LSTM models\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # +1 to ensure index 0 for padding in sequence models\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # Pad sequences (makes sequences uniform in length for neural networks by adding 0 (padding) or cut off excess (truncation)\n",
    "    max_length = 100  # Can adjust text length analysis\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "    \n",
    "    # Create Word2Vec embeddings\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Create embedding matrix for pre-trained embeddings\n",
    "    embedding_dim = 100\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    \n",
    "    # Model 1: CNN with Word2Vec embeddings\n",
    "    print(\"\\nTraining CNN model with Word2Vec embeddings...\")\n",
    "    cnn_model = build_cnn_model(vocab_size, embedding_dim, embedding_matrix)\n",
    "    cnn_results = train_evaluate_model(\n",
    "        cnn_model, X_train_pad, y_train, X_test_pad, y_test_orig, \n",
    "        batch_size=32, epochs=15, model_name=\"CNN_Word2Vec\"\n",
    "    )\n",
    "\n",
    "    # Model 2: LSTM with Word2Vec embeddings\n",
    "    print(\"\\nTraining LSTM model with Word2Vec embeddings...\")\n",
    "    lstm_model = build_lstm_model(vocab_size, embedding_dim, embedding_matrix)\n",
    "    lstm_results = train_evaluate_model(\n",
    "        lstm_model, X_train_pad, y_train, X_test_pad, y_test_orig, \n",
    "        batch_size=32, epochs=15, model_name=\"LSTM_Word2Vec\"\n",
    "    )\n",
    "    \n",
    "    # Model 3: BERT-based model (Transfer Learning)\n",
    "    print(\"\\nTraining BERT-based model (Transfer Learning)...\")\n",
    "    bert_max_length = 128\n",
    "    bert_model = build_bert_model(bert_max_length)\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Tokenize data\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
    "    test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # Extract input_ids and attention_mask\n",
    "    X_train_bert_ids = train_encodings[\"input_ids\"]\n",
    "    X_train_bert_masks = train_encodings[\"attention_mask\"]\n",
    "    X_test_bert_ids = test_encodings[\"input_ids\"]\n",
    "    X_test_bert_masks = test_encodings[\"attention_mask\"]\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_bert_ids = np.array(X_train_bert_ids)\n",
    "    X_train_bert_masks = np.array(X_train_bert_masks)\n",
    "    X_test_bert_ids = np.array(X_test_bert_ids)\n",
    "    X_test_bert_masks = np.array(X_test_bert_masks)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    bert_results = train_evaluate_model(\n",
    "        bert_model, \n",
    "        (X_train_bert_ids, X_train_bert_masks), y_train,\n",
    "        (X_test_bert_ids, X_test_bert_masks), y_test_orig,\n",
    "        batch_size=16, epochs=5, model_name=\"BERT_Transfer\"\n",
    "    )\n",
    "    \n",
    "    # Compare all models\n",
    "    all_model_results = {\n",
    "        'CNN_Word2Vec': cnn_results,\n",
    "        'LSTM_Word2Vec': lstm_results,\n",
    "        'BERT_Transfer': bert_results\n",
    "    }\n",
    "\n",
    "    # Compare and visualize model performances\n",
    "    comparison_table = compare_models(all_model_results)\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(comparison_table)\n",
    "\n",
    "    # Print classification reports for each model\n",
    "    print(\"\\nCNN Model Classification Report:\")\n",
    "    print(cnn_results['report'])\n",
    "\n",
    "    print(\"\\nLSTM Model Classification Report:\")\n",
    "    print(lstm_results['report'])\n",
    "\n",
    "    print(\"\\nBERT Model Classification Report:\")\n",
    "    print(bert_results['report'])\n",
    "\n",
    "    # Saving Models\n",
    "    # 1. Save Word2Vec (vectors and vocabulary)\n",
    "    w2v_model.save(\"models/word2vec.model\")\n",
    "    np.save(\"models/embedding_matrix.npy\", embedding_matrix)\n",
    "    # 2. Save CNN (architecture 'layers', weights, optimizer state)\n",
    "    cnn_model.save(\"models/cnn_word2vec.keras\")\n",
    "    # 3. Save LSTM \n",
    "    lstm_model.save(\"models/lstm_word2vec.keras\")\n",
    "    # 4. Save Bert-based model\n",
    "    bert_model.save(\"models/bert_transfer.keras\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a40c8a-b2ff-4464-9c8d-1d26731fa7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ML)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
